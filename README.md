# Lip-reading-project

The process being referred to here is a machine learning project that involves the recognition of spoken words from video inputs. The first step in this process is to read the video input, which is typically a sequence of frames captured from a camera or video file. To reduce the amount of data to be processed, the frames are converted into a compressed format known as Graphics Interchange Format (.gif).

Once the video input has been transformed into a gif format, it is passed to the machine learning model. This model has been trained on large amounts of data to identify spoken words from video inputs. The model processes the gif file and makes a prediction on what the spoken words are. The model utilizes advanced algorithms such as Conv3d(CNN algorithm), MaxPool3D and Long Short-Term Memory (LSTM) networks to make these predictions.

The objective of this machine learning project is to develop a real-time system that can accurately detect spoken words from video inputs even in the absence of audio, thereby ensuring its reliability.

